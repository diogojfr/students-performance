{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"End to End Machine Learning Project This documentation is inspired by a series of videos from Krish Naik YouTube channel . A machine learning algorithm is developed to predict student performance on the test according to the features available in the dataset. The main goal of this data science project is to understand the entire data lifecycle, using pipelines and coding best practices to perform steps such as data ingestion, preprocessing, EDA, model training, evaluation, and deployment.","title":"Home"},{"location":"#end-to-end-machine-learning-project","text":"This documentation is inspired by a series of videos from Krish Naik YouTube channel . A machine learning algorithm is developed to predict student performance on the test according to the features available in the dataset. The main goal of this data science project is to understand the entire data lifecycle, using pipelines and coding best practices to perform steps such as data ingestion, preprocessing, EDA, model training, evaluation, and deployment.","title":"End to End Machine Learning Project"},{"location":"dataingestion/","text":"Data Ingestion The DataIngestionConfig class creates the path for the train, test, and raw data. class DataIngestionConfig: train_data_path: str=os.path.join('artifacts',\"train.csv\") test_data_path: str=os.path.join('artifacts',\"test.csv\") raw_data_path: str=os.path.join('artifacts',\"data.csv\") In the DataIngestion class, the first function ( __init__ ) gets the path for the train, test and raw data using DataIngestionConfig . The initiate_data_ingestion reads the original data set as a dataframe and splits the data into train and test data. This function returns the path to the split data. class DataIngestion: def __init__(self): self.ingestion_config=DataIngestionConfig() def initiate_data_ingestion(self): logging.info(\"Entered the data ingestion methods or components\") try: df=pd.read_csv('notebook\\data\\stud.csv') logging.info('Read the dataset as dataframe') os.makedirs(os.path.dirname(self.ingestion_config.train_data_path), exist_ok=True) df.to_csv(self.ingestion_config.raw_data_path, index=False, header=True) logging.info('Train test split initiated') train_set, test_set = train_test_split(df, test_size=0.2,random_state=42) train_set.to_csv(self.ingestion_config.train_data_path, index=False, header=True) test_set.to_csv(self.ingestion_config.test_data_path, index=False, header=True) logging.info('Ingestion of the data is completed') return( self.ingestion_config.train_data_path, self.ingestion_config.test_data_path ) except Exception as e: raise CustomException(e,sys)","title":"Data ingestion"},{"location":"dataingestion/#data-ingestion","text":"The DataIngestionConfig class creates the path for the train, test, and raw data. class DataIngestionConfig: train_data_path: str=os.path.join('artifacts',\"train.csv\") test_data_path: str=os.path.join('artifacts',\"test.csv\") raw_data_path: str=os.path.join('artifacts',\"data.csv\") In the DataIngestion class, the first function ( __init__ ) gets the path for the train, test and raw data using DataIngestionConfig . The initiate_data_ingestion reads the original data set as a dataframe and splits the data into train and test data. This function returns the path to the split data. class DataIngestion: def __init__(self): self.ingestion_config=DataIngestionConfig() def initiate_data_ingestion(self): logging.info(\"Entered the data ingestion methods or components\") try: df=pd.read_csv('notebook\\data\\stud.csv') logging.info('Read the dataset as dataframe') os.makedirs(os.path.dirname(self.ingestion_config.train_data_path), exist_ok=True) df.to_csv(self.ingestion_config.raw_data_path, index=False, header=True) logging.info('Train test split initiated') train_set, test_set = train_test_split(df, test_size=0.2,random_state=42) train_set.to_csv(self.ingestion_config.train_data_path, index=False, header=True) test_set.to_csv(self.ingestion_config.test_data_path, index=False, header=True) logging.info('Ingestion of the data is completed') return( self.ingestion_config.train_data_path, self.ingestion_config.test_data_path ) except Exception as e: raise CustomException(e,sys)","title":"Data Ingestion"},{"location":"datatranform/","text":"Data Trasformation The data transformation is initiated by setting the path where the preprocessor object will be stored. This task is performed by the DataTransformationConfig class. class DataTransformationConfig: preprocessor_obj_file_path=os.path.join('artifacts',\"preprocessor.pkl\") In the DataTransformation class, the get_data_transformer_obj function returns the preprocessor object that will perform the imputation, scaling, and encoding of the categorical/numeric variables. The second function, initiate_data_transformation , takes as input arguments the paths for the train/test data. This function applies the preprocessor object to the train/test data frame and returns arrays with the transformed data. The preprocessor object is saved as a pickle file using the save_object function. The save_object function is available in the utils.py script. class DataTransformation: def __init__(self): self.data_transformation_config = DataTransformationConfig() def get_data_transformer_obj(self): \"\"\" This function is responsible for data transformation \"\"\" try: numerical_columns = [\"writing_score\", \"reading_score\"] categorical_columns = [ \"gender\", \"race_ethnicity\", \"parental_level_of_education\", \"lunch\", \"test_preparation_course\", ] # Imputing and scaling the numerical columns num_pipeline= Pipeline( steps=[ (\"imputer\",SimpleImputer(strategy=\"median\")), (\"scaler\",StandardScaler()) ] ) #Imputing, encoding and scaling the categorical columns cat_pipeline=Pipeline( steps=[ (\"imputer\",SimpleImputer(strategy=\"most_frequent\")), (\"one_hot_encoder\",OneHotEncoder()), (\"scaler\",StandardScaler(with_mean=False)) ] ) logging.info(f\"Categorical columns: {categorical_columns}\") logging.info(f\"Numerical columns: {numerical_columns}\") preprocessor=ColumnTransformer( [ (\"num_pipeline\",num_pipeline,numerical_columns), (\"cat_pipelines\",cat_pipeline,categorical_columns) ] ) # retuning the modified dataset return preprocessor except Exception as e: raise CustomException(e, sys) def initiate_data_transformation(self, train_path, test_path): try: train_df = pd.read_csv(train_path) test_df = pd.read_csv(test_path) logging.info('Reading train and test data completed') logging.info('Obtaining preprocessing object') preprocessing_obj=self.get_data_transformer_obj() target_column_name = \"math_score\" numerical_columns = [\"writing_score\", \"reading_score\"] # getting X_train input_feature_train_df = train_df.drop(columns=[target_column_name],axis=1) # getting y_train target_feature_train_df=train_df[target_column_name] # getting X_test input_feature_test_df=test_df.drop(columns=[target_column_name],axis=1) # getting y_test target_feature_test_df=test_df[target_column_name] logging.info( f\"Applying preprocessing object on training dataframe and testing dataframe.\" ) input_feature_train_arr=preprocessing_obj.fit_transform(input_feature_train_df) input_feature_test_arr=preprocessing_obj.transform(input_feature_test_df) train_arr = np.c_[ input_feature_train_arr,np.array(target_feature_train_df) ] test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)] save_object( file_path=self.data_transformation_config.preprocessor_obj_file_path, obj=preprocessing_obj ) return ( train_arr, test_arr, self.data_transformation_config.preprocessor_obj_file_path, ) except Exception as e: raise CustomException(e,sys)","title":"Data transformation (pipelines)"},{"location":"datatranform/#data-trasformation","text":"The data transformation is initiated by setting the path where the preprocessor object will be stored. This task is performed by the DataTransformationConfig class. class DataTransformationConfig: preprocessor_obj_file_path=os.path.join('artifacts',\"preprocessor.pkl\") In the DataTransformation class, the get_data_transformer_obj function returns the preprocessor object that will perform the imputation, scaling, and encoding of the categorical/numeric variables. The second function, initiate_data_transformation , takes as input arguments the paths for the train/test data. This function applies the preprocessor object to the train/test data frame and returns arrays with the transformed data. The preprocessor object is saved as a pickle file using the save_object function. The save_object function is available in the utils.py script. class DataTransformation: def __init__(self): self.data_transformation_config = DataTransformationConfig() def get_data_transformer_obj(self): \"\"\" This function is responsible for data transformation \"\"\" try: numerical_columns = [\"writing_score\", \"reading_score\"] categorical_columns = [ \"gender\", \"race_ethnicity\", \"parental_level_of_education\", \"lunch\", \"test_preparation_course\", ] # Imputing and scaling the numerical columns num_pipeline= Pipeline( steps=[ (\"imputer\",SimpleImputer(strategy=\"median\")), (\"scaler\",StandardScaler()) ] ) #Imputing, encoding and scaling the categorical columns cat_pipeline=Pipeline( steps=[ (\"imputer\",SimpleImputer(strategy=\"most_frequent\")), (\"one_hot_encoder\",OneHotEncoder()), (\"scaler\",StandardScaler(with_mean=False)) ] ) logging.info(f\"Categorical columns: {categorical_columns}\") logging.info(f\"Numerical columns: {numerical_columns}\") preprocessor=ColumnTransformer( [ (\"num_pipeline\",num_pipeline,numerical_columns), (\"cat_pipelines\",cat_pipeline,categorical_columns) ] ) # retuning the modified dataset return preprocessor except Exception as e: raise CustomException(e, sys) def initiate_data_transformation(self, train_path, test_path): try: train_df = pd.read_csv(train_path) test_df = pd.read_csv(test_path) logging.info('Reading train and test data completed') logging.info('Obtaining preprocessing object') preprocessing_obj=self.get_data_transformer_obj() target_column_name = \"math_score\" numerical_columns = [\"writing_score\", \"reading_score\"] # getting X_train input_feature_train_df = train_df.drop(columns=[target_column_name],axis=1) # getting y_train target_feature_train_df=train_df[target_column_name] # getting X_test input_feature_test_df=test_df.drop(columns=[target_column_name],axis=1) # getting y_test target_feature_test_df=test_df[target_column_name] logging.info( f\"Applying preprocessing object on training dataframe and testing dataframe.\" ) input_feature_train_arr=preprocessing_obj.fit_transform(input_feature_train_df) input_feature_test_arr=preprocessing_obj.transform(input_feature_test_df) train_arr = np.c_[ input_feature_train_arr,np.array(target_feature_train_df) ] test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)] save_object( file_path=self.data_transformation_config.preprocessor_obj_file_path, obj=preprocessing_obj ) return ( train_arr, test_arr, self.data_transformation_config.preprocessor_obj_file_path, ) except Exception as e: raise CustomException(e,sys)","title":"Data Trasformation"},{"location":"flaskpred/","text":"Prediction using Flask The predic_pipeline.py script contains the PredictPipeline and CustomData classes. In the CustomData class, gets the feature values inserted by the user in the html page and creates a dataframe with this information. class CustomData: def __init__(self, gender: str, race_ethnicity: str, parental_level_of_education, lunch: str, test_preparation_course: str, reading_score: int, writing_score: int ): self.gender = gender self.race_ethnicity = race_ethnicity self.parental_level_of_education = parental_level_of_education self.lunch = lunch self.test_preparation_course = test_preparation_course self.reading_score = reading_score self.writing_score = writing_score def get_data_as_dataframe(self): try: custom_data_input_dict = { 'gender':[self.gender], 'race_ethnicity':[self.race_ethnicity], 'parental_level_of_education':[self.parental_level_of_education], 'lunch':[self.lunch], 'test_preparation_course':[self.test_preparation_course], 'reading_score':[self.reading_score], 'writing_score':[self.writing_score] } return pd.DataFrame(custom_data_input_dict) except Exception as e: raise CustomException(e,sys) In the PredictPipeline class, the predict function gets the dataframe with the feature values inserted by the user and returns the prediction score. The web application in the app.py script prints the result to the html page. class PredictPipeline: def __init__(self): pass def predict(self, features): try: model_path='artifacts\\model.pkl' preprocessor_path='artifacts\\preprocessor.pkl' model=load_object(file_path=model_path) preprocessor=load_object(file_path=preprocessor_path) data_scaled = preprocessor.transform(features) preds = model.predict(data_scaled) return preds except Exception as e: raise CustomException(e,sys)","title":"Prediction using Flask"},{"location":"flaskpred/#prediction-using-flask","text":"The predic_pipeline.py script contains the PredictPipeline and CustomData classes. In the CustomData class, gets the feature values inserted by the user in the html page and creates a dataframe with this information. class CustomData: def __init__(self, gender: str, race_ethnicity: str, parental_level_of_education, lunch: str, test_preparation_course: str, reading_score: int, writing_score: int ): self.gender = gender self.race_ethnicity = race_ethnicity self.parental_level_of_education = parental_level_of_education self.lunch = lunch self.test_preparation_course = test_preparation_course self.reading_score = reading_score self.writing_score = writing_score def get_data_as_dataframe(self): try: custom_data_input_dict = { 'gender':[self.gender], 'race_ethnicity':[self.race_ethnicity], 'parental_level_of_education':[self.parental_level_of_education], 'lunch':[self.lunch], 'test_preparation_course':[self.test_preparation_course], 'reading_score':[self.reading_score], 'writing_score':[self.writing_score] } return pd.DataFrame(custom_data_input_dict) except Exception as e: raise CustomException(e,sys) In the PredictPipeline class, the predict function gets the dataframe with the feature values inserted by the user and returns the prediction score. The web application in the app.py script prints the result to the html page. class PredictPipeline: def __init__(self): pass def predict(self, features): try: model_path='artifacts\\model.pkl' preprocessor_path='artifacts\\preprocessor.pkl' model=load_object(file_path=model_path) preprocessor=load_object(file_path=preprocessor_path) data_scaled = preprocessor.transform(features) preds = model.predict(data_scaled) return preds except Exception as e: raise CustomException(e,sys)","title":"Prediction using Flask"},{"location":"githubsetup/","text":"GitHub and code setup Setup.py is responsible for building this machine learning application as a package. For example, it allows this application to be built as a package that is deployed in PyPi, and other users can download and install this machine learning application. The setup() function provides the general information about the entire project. setup( name='students-performance', version='0.0.1', author='Diogo', author_email='diogojfr1@gmail.com', packages=find_packages(), install_requires = get_requirements('requirements.txt') ) The __init__.py in the src (source) folder allows that folder to be built as a package when setup.py is run. The requirements.txt file contains all the libraries required for the project. For the first building, it is required to add a line with -e . . The function get_requirements takes a string (the path of requirements.txt) and and returns a list of libraries. The argument file_path is expected to be of type \u00b4str\u00b4 and the return type \u00b4List\u00b4 of strings. HYPHEN_E_DOT='-e .' def get_requirements(file_path:str)->List[str]: ''' This function will return the list of requirements ''' requirements=[] with open(file_path) as file_obj: requirements=file_obj.readlines() requirements=[req.replace(\"\\n\",\"\") for req in requirements ] if HYPHEN_E_DOT in requirements: requirements.remove(HYPHEN_E_DOT) return requirements Building the package: pip install -r requirements.txt Strcture of the project: |__src/ # project source folder | |__components/ # modules for the project | | |__ __init__.py | | |__data_ingestion.py | | |__data_transformation.py | | |__model_trainer.py | |__pipeline/ # pipiles for training and prediction | | |__ __init__.py | | |__predict_pipeline.py | | |__train_pipeline.py | |__ __init__.py | |__logger.py | |__exception.py | |__utils.py |__setup.py |__app.py |__artifacts/ # train and test data |__notebook/ # EDA in jupyter notebook | |__data/ # raw dataset |__templates/ # templates for html pages |__requirements.txt # libraries","title":"GitHub and code setup"},{"location":"githubsetup/#github-and-code-setup","text":"Setup.py is responsible for building this machine learning application as a package. For example, it allows this application to be built as a package that is deployed in PyPi, and other users can download and install this machine learning application. The setup() function provides the general information about the entire project. setup( name='students-performance', version='0.0.1', author='Diogo', author_email='diogojfr1@gmail.com', packages=find_packages(), install_requires = get_requirements('requirements.txt') ) The __init__.py in the src (source) folder allows that folder to be built as a package when setup.py is run. The requirements.txt file contains all the libraries required for the project. For the first building, it is required to add a line with -e . . The function get_requirements takes a string (the path of requirements.txt) and and returns a list of libraries. The argument file_path is expected to be of type \u00b4str\u00b4 and the return type \u00b4List\u00b4 of strings. HYPHEN_E_DOT='-e .' def get_requirements(file_path:str)->List[str]: ''' This function will return the list of requirements ''' requirements=[] with open(file_path) as file_obj: requirements=file_obj.readlines() requirements=[req.replace(\"\\n\",\"\") for req in requirements ] if HYPHEN_E_DOT in requirements: requirements.remove(HYPHEN_E_DOT) return requirements Building the package: pip install -r requirements.txt Strcture of the project: |__src/ # project source folder | |__components/ # modules for the project | | |__ __init__.py | | |__data_ingestion.py | | |__data_transformation.py | | |__model_trainer.py | |__pipeline/ # pipiles for training and prediction | | |__ __init__.py | | |__predict_pipeline.py | | |__train_pipeline.py | |__ __init__.py | |__logger.py | |__exception.py | |__utils.py |__setup.py |__app.py |__artifacts/ # train and test data |__notebook/ # EDA in jupyter notebook | |__data/ # raw dataset |__templates/ # templates for html pages |__requirements.txt # libraries","title":"GitHub and code setup"},{"location":"logexception/","text":"Logging and exception handling exception.py In this project, the exception.py file was created to provide a custom exception function to inform the user about errors during execution. The CustomException() class uses the error_message_detail() to return an error message and details about the error. class CustomException(Exception): def __init__(self, error_message, error_detail:sys): super().__init__(error_message) self.error_message = error_message_detail(error_message, error_detail=error_detail) def __str__(self): return self.error_message The error_message_detail() function takes two arguments: the error and its details (from the sys library). This function returns a string containing the script name, line number and a message about the error. def error_message_detail(error, error_detail:sys): _,_,exc_tb = error_detail.exc_info() file_name = exc_tb.tb_frame.f_code.co_filename error_message = \"Error in pyhton script name [{0}] line number [{1}] error message [{2}]\".format( file_name, exc_tb.tb_lineno, str(error) ) return error_message Usage example: if __name__ == \"__main__\": try: a=1/0 except Exception as e: raise CustomException(e,sys) logger.py The logger.py file uses the logging module of Python to generate a log. The user provides a message to the logging.info() function and it returns information about the date/time and the message provided by the user. logging.basicConfig( filename=LOG_FILE_PATH, format=\"[ %(asctime)s ] %(lineno)d %(name)s - %(levelname)s - %(message)s\", level=logging.INFO, ) Usage example: if __name__ == \"__main__\": logging.info(\"Logging has started\")","title":"Logging and exception handling"},{"location":"logexception/#logging-and-exception-handling","text":"","title":"Logging and exception handling"},{"location":"logexception/#exceptionpy","text":"In this project, the exception.py file was created to provide a custom exception function to inform the user about errors during execution. The CustomException() class uses the error_message_detail() to return an error message and details about the error. class CustomException(Exception): def __init__(self, error_message, error_detail:sys): super().__init__(error_message) self.error_message = error_message_detail(error_message, error_detail=error_detail) def __str__(self): return self.error_message The error_message_detail() function takes two arguments: the error and its details (from the sys library). This function returns a string containing the script name, line number and a message about the error. def error_message_detail(error, error_detail:sys): _,_,exc_tb = error_detail.exc_info() file_name = exc_tb.tb_frame.f_code.co_filename error_message = \"Error in pyhton script name [{0}] line number [{1}] error message [{2}]\".format( file_name, exc_tb.tb_lineno, str(error) ) return error_message Usage example: if __name__ == \"__main__\": try: a=1/0 except Exception as e: raise CustomException(e,sys)","title":"exception.py"},{"location":"logexception/#loggerpy","text":"The logger.py file uses the logging module of Python to generate a log. The user provides a message to the logging.info() function and it returns information about the date/time and the message provided by the user. logging.basicConfig( filename=LOG_FILE_PATH, format=\"[ %(asctime)s ] %(lineno)d %(name)s - %(levelname)s - %(message)s\", level=logging.INFO, ) Usage example: if __name__ == \"__main__\": logging.info(\"Logging has started\")","title":"logger.py"},{"location":"mdltraineval/","text":"Model Training and Evaluation The ModelTrainerConfig class creates the path where the model (pickle file) will be stored. class ModelTrainerConfig: trained_model_file_path = os.path.join('artifacts','model.pkl') The ModelTrainer starts by instantiating the ModelTrainerConfig class, i.e. getting the path where the model will be stored. After that, the initiate_model_trainer function gets the arrays of training and test data and performs model training for several models using the hyperparameters presented in the script. The evaluate_models function is used to get a report of the model results. The evaluate_models function is available in the utils.py script. The model with the best score is saved as a pickle file using the save_object function. class ModelTrainer: def __init__(self): self.model_trainer_config = ModelTrainerConfig() def initiate_model_trainer(self, train_array, test_array): try: logging.info('Spliting traning and test input data') X_train,y_train,X_test,y_test=( train_array[:,:-1], train_array[:,-1], test_array[:,:-1], test_array[:,-1] ) models = { \"Random Forest\": RandomForestRegressor(), \"Decision Tree\": DecisionTreeRegressor(), \"Gradient Boosting\": GradientBoostingRegressor(), \"Linear Regression\": LinearRegression(), \"XGBRegressor\": XGBRegressor(), \"CatBoosting Regressor\": CatBoostRegressor(verbose=False), \"AdaBoost Regressor\": AdaBoostRegressor(), } params={ \"Decision Tree\": { 'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'], # 'splitter':['best','random'], # 'max_features':['sqrt','log2'], }, \"Random Forest\":{ # 'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'], # 'max_features':['sqrt','log2',None], 'n_estimators': [8,16,32,64,128,256] }, \"Gradient Boosting\":{ # 'loss':['squared_error', 'huber', 'absolute_error', 'quantile'], 'learning_rate':[.1,.01,.05,.001], 'subsample':[0.6,0.7,0.75,0.8,0.85,0.9], # 'criterion':['squared_error', 'friedman_mse'], # 'max_features':['auto','sqrt','log2'], 'n_estimators': [8,16,32,64,128,256] }, \"Linear Regression\":{}, \"XGBRegressor\":{ 'learning_rate':[.1,.01,.05,.001], 'n_estimators': [8,16,32,64,128,256] }, \"CatBoosting Regressor\":{ 'depth': [6,8,10], 'learning_rate': [0.01, 0.05, 0.1], 'iterations': [30, 50, 100] }, \"AdaBoost Regressor\":{ 'learning_rate':[.1,.01,0.5,.001], # 'loss':['linear','square','exponential'], 'n_estimators': [8,16,32,64,128,256] } } model_report:dict=evaluate_models(X_train=X_train,y_train=y_train,X_test=X_test,y_test=y_test,models=models,param=params) # get the best model score best_model_score = max(sorted(model_report.values())) # get the best model name best_model_name = list(model_report.keys())[ list(model_report.values()).index(best_model_score) ] best_model = models[best_model_name] if best_model_score < 0.6: raise CustomException('No best model found') logging.info(f'Best model found on both training and testing dataset') save_object( file_path=self.model_trainer_config.trained_model_file_path, obj=best_model ) predicted=best_model.predict(X_test) r2_score_value = r2_score(y_test,predicted) return r2_score_value except Exception as e: raise CustomException(e,sys)","title":"Model training and evaluation"},{"location":"mdltraineval/#model-training-and-evaluation","text":"The ModelTrainerConfig class creates the path where the model (pickle file) will be stored. class ModelTrainerConfig: trained_model_file_path = os.path.join('artifacts','model.pkl') The ModelTrainer starts by instantiating the ModelTrainerConfig class, i.e. getting the path where the model will be stored. After that, the initiate_model_trainer function gets the arrays of training and test data and performs model training for several models using the hyperparameters presented in the script. The evaluate_models function is used to get a report of the model results. The evaluate_models function is available in the utils.py script. The model with the best score is saved as a pickle file using the save_object function. class ModelTrainer: def __init__(self): self.model_trainer_config = ModelTrainerConfig() def initiate_model_trainer(self, train_array, test_array): try: logging.info('Spliting traning and test input data') X_train,y_train,X_test,y_test=( train_array[:,:-1], train_array[:,-1], test_array[:,:-1], test_array[:,-1] ) models = { \"Random Forest\": RandomForestRegressor(), \"Decision Tree\": DecisionTreeRegressor(), \"Gradient Boosting\": GradientBoostingRegressor(), \"Linear Regression\": LinearRegression(), \"XGBRegressor\": XGBRegressor(), \"CatBoosting Regressor\": CatBoostRegressor(verbose=False), \"AdaBoost Regressor\": AdaBoostRegressor(), } params={ \"Decision Tree\": { 'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'], # 'splitter':['best','random'], # 'max_features':['sqrt','log2'], }, \"Random Forest\":{ # 'criterion':['squared_error', 'friedman_mse', 'absolute_error', 'poisson'], # 'max_features':['sqrt','log2',None], 'n_estimators': [8,16,32,64,128,256] }, \"Gradient Boosting\":{ # 'loss':['squared_error', 'huber', 'absolute_error', 'quantile'], 'learning_rate':[.1,.01,.05,.001], 'subsample':[0.6,0.7,0.75,0.8,0.85,0.9], # 'criterion':['squared_error', 'friedman_mse'], # 'max_features':['auto','sqrt','log2'], 'n_estimators': [8,16,32,64,128,256] }, \"Linear Regression\":{}, \"XGBRegressor\":{ 'learning_rate':[.1,.01,.05,.001], 'n_estimators': [8,16,32,64,128,256] }, \"CatBoosting Regressor\":{ 'depth': [6,8,10], 'learning_rate': [0.01, 0.05, 0.1], 'iterations': [30, 50, 100] }, \"AdaBoost Regressor\":{ 'learning_rate':[.1,.01,0.5,.001], # 'loss':['linear','square','exponential'], 'n_estimators': [8,16,32,64,128,256] } } model_report:dict=evaluate_models(X_train=X_train,y_train=y_train,X_test=X_test,y_test=y_test,models=models,param=params) # get the best model score best_model_score = max(sorted(model_report.values())) # get the best model name best_model_name = list(model_report.keys())[ list(model_report.values()).index(best_model_score) ] best_model = models[best_model_name] if best_model_score < 0.6: raise CustomException('No best model found') logging.info(f'Best model found on both training and testing dataset') save_object( file_path=self.model_trainer_config.trained_model_file_path, obj=best_model ) predicted=best_model.predict(X_test) r2_score_value = r2_score(y_test,predicted) return r2_score_value except Exception as e: raise CustomException(e,sys)","title":"Model Training and Evaluation"}]}